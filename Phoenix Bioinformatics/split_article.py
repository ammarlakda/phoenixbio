from summarizer import summarizer
from nltk import word_tokenize
import nltk
nltk.download('punkt')



def summarize(chunked_article):
  summary = summarizer(chunked_article)
  return summary

def split_article(Article):
  """
  Split article into chunks and sends it to the gpt 3 summarizer and outputs the final string which is summarized
  parameters: article
  return: string
  """
  #Maximum sequence length 
  K = 800
  #Total number of tokens in Document
  N = len(word_tokenize(Article))
  words = word_tokenize(Article)
  #Let I be the number of sequences of K tokens or less in Document
  paddingSize = 50
  I = (N//K)
  i =0
  chunks = [""] * (I + 1)
  
  while i < N:
    # Padding start
    if i != 0:
      if i % K == 0:
        counter = paddingSize
        while counter > 0:
          chunks[i//K] += words[i-counter] + " "
          counter = counter - 1



    chunks[i//K] += " " + words[i]
    
    # Padding end
    if i % K == K-1 and i<N-paddingSize:
        counter = 1
        while counter <= paddingSize:
          chunks[i//K] += words[i+counter] + " "
          counter = counter + 1

    i += 1
  master_string = ""

  for i in range(len(chunks)):
    
    temp = summarize(chunks[i])

    master_string += temp + "\n"

  
  return master_string

# print(split_article("ExplaiNN is a glass box deep learning model for genomicsExplaiNN is a fully interpretable and transparent deep learning approach for genomic taskstrained on one-hot encoded sequences inspired by NAMs1. Predictions are computed as alinear combination of multiple independent CNNs (hereafter referred to as units), each ofwhich consisting of one convolutional layer with a single filter and two fully connected layers(Fig. 1A). ExplaiNN provides global interpretability, as the filter of each unit can be readilymapped to a TF profile from JASPAR using Tomtom24 (Methods), thus assigning a biologicalinterpretation to that unit. Besides, the weights of each unit from the final linear layer can bevisualized, akin to linear models. ExplaiNN also provides local interpretability by multiplyingthe output of each unit by the weight of that unit for each input sequence (hereafter referredto as unit importance scores; Methods).As a proof of concept, we applied ExplaiNN to predict the binding of 50 TFs to openchromatin regions (OCRs) from a reference dataset describing the binding of 163 TFs to>1.8M 200-bp long OCRs that we repurposed for this study (Methods). A keyhyperparameter in ExplaiNN is the number of independent units to be used. To assess theimpact of this hyperparameter on model performance, we trained multiple ExplaiNN modelsusing increasing numbers of units (from 1 to 200). As expected, the performance ofExplaiNN improved with the number of units used, plateauing at around 100 units (Fig. 1B).For comparison, we evaluated four additional models on the same dataset (Methods): aCNN with one convolutional layer (CNN1); a CNN1 with exponential activation function(CNN1Exp); a deep CNN with three convolutional layers (DeepCNN); and DanQ7, a hybriddeep learning model with convolutional and recurrent layers. Although simpler, ExplaiNNoutperformed all three CNN models as measured by average area under the precision-recallcurve (AUPRC) and, when using more than 100 units, nearly reached the performance ofthe more complex DanQ (Fig. 1B). Focusing on the ExplaiNN model trained with 100 units, itoutperformed the DeepCNN model for most TFs, performing only slightly worse than DanQ(Fig. 1C).Next, we visualized the filters of each ExplaiNN model and assigned them TF binding modesfrom JASPAR, which we defined based on the hierarchically clustered groups ofDNA-binding profiles included in the database (Table S1; Methods). As with performance,the number of binding modes recovered by the model increased with the number of unitsused (Fig. 1D). For comparison, we provide the number of binding modes recovered by the5made available under aCC-BY-NC-ND 4.0 International license.(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It isbioRxiv preprint doi: https://doi.org/10.1101/2022.05.20.492818; this version posted May 22, 2022. The copyright holder for this preprintDeepCNN and DanQ when applying filter visualization, as we did with ExplaiNN, or whenusing TF-MoDISco22 clustering on DeepLIFT21 attribution scores (Methods). Out of 33different binding modes associated with the set of 50 TFs analyzed, ExplaiNN modelstrained with 100 and 300 units recovered 19 and 21, respectively, which is a similar numberas DanQ (19 when applying filter visualization and 20 when using DeepLIFT andTF-MoDISco) and greater than obtained for the DeepCNN model (Fig. 1E).An advantageous feature of ExplaiNN is that one can readily visualize the final linear layerweights for global interpretation purposes (Fig. 1F; Methods). For instance, units with filtersannotated as FOX motifs had high positive weights for predicting the FOXA1 class. Similarly,CEBP-, CTCF-, and Ets-like units had high positive weights associated with predicting theclasses of CEBP factors, CTCF, and Ets family members, respectively. However, some unitshad negative weights for predicting the class of their annotated TFs (Fig. 1F; highlighted witharrows). To delve further into the contribution of each unit to the prediction of each class, wecomputed unit importance scores (Methods). Visualization of the importance scores of aFOX-like unit in a heatmap confirmed its importance for predicting the FOXA1 and ARclasses (Fig. 1G; top panel), in agreement with the observation that FOXA1 helps shape ARsignaling in prostate cells25. Visualizing unit importance scores also revealed why severalCTCF-like units had negative weights for predicting the CTCF class: the importances ofthese units for the CTCF class were negligible, suggesting that the model was not usingthem to make predictions for that class (Fig. S1). The same was true for units annotated asCEBP and Ets with negative weights for those classes (Fig. S1). Finally, we calculated theimpact from nullifying the FOX-like filters in the DanQ model one at a time (Fig. 1G; bottompanel; Methods) and, as expected, the impact scores from the filter nullification processwere consistent with the unit importance scores.Taken together, these analyses demonstrated that ExplaiNN performs comparably to morecomplex models, at least for TF binding prediction. In addition, ExplaiNN provided local andglobal interpretation quickly and readily compared to using DeepLIFT followed byTF-MoDISco or filter visualization and nullification.ExplaiNN cannot capture nonlinear interactions between motifsGiven the architecture of ExplaiNN, in which each unit filter is independent of the rest, weexpected that it would not be able to learn nonlinear interactions between pairs of TF motifs,including additive and multiplicative effects26. In contrast, DeepSTARR is a CNN trained onSTARR-seq data to predict the activities of Drosophila developmental and housekeeping6 made available under aCC-BY-NC-ND 4.0 International license.(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It isbioRxiv preprint doi: https://doi.org/10.1101/2022.05.20.492818; this version posted May 22, 2022. The copyright holder for this preprintenhancers that can capture these types of interactions27. For each pair of motifs analyzed,the authors performed a distance dependence analysis by sliding one along randomlygenerated sequences within which they embedded the second motif. Accounting for additive effects, they observed that the output of DeepSTARR increased when the two motifs wereproximal, suggesting that the model had learned nonlinear interactions. To check whetherthis was also the case for ExplaiNN, we trained it on the same dataset as used forDeepSTARR and compared their performances by calculating the Pearson correlationcoefficient (PCC) between predicted and actual enhancer activities (Methods). ExplaiNNperformed worse than DeepSTARR on both developmental (PCC = 0.61 vs. 0.68) andhousekeeping (PCC = 0.71 vs. 0.74) enhancers, which we attributed to the greater presenceof nonlinear interactions in this particular dataset. Next, following the specifications fromDeepSTARR, we performed a distance dependence analysis between the housekeepingTFs Dref, Ohler1, and Ohler6 (Methods). As a negative control, we slid the 5-mer GGGCT.As expected, DeepSTARR was able to learn distance dependencies between the threemotifs (Fig. S2). This was not true for ExplaiNN: during the analysis, the resulting modeloutputs using the three motifs were similar to sliding GGGCT (Fig. S2). Therefore, asanticipated based on the model architecture, ExplaiNN is not suitable for nonlinear taskssuch as detecting motif interactions.ExplaiNN learns high-quality motifs comparable to de novo motif discovery toolsDe novo motif discovery methods continue to emerge and improve28-32. With the dramaticescalation in the size of datasets, the execution time of these methods is increasingly alimitation. Furthermore, many de novo motif discovery methods are assay-specific, asexemplified by the DREAM5 challenge evaluation on protein binding microarray (PBM)data33, requiring an extensive adjustment of parameters for their application to differentassays. We sought to explore the capacity of ExplaiNN for efficient de novo motif discoverywithin a unified platform. For each of the 163 TFs from the previous dataset, we trained amodel with 100 units and then visualized the filters and importance scores of each unit,resulting in 100 PWMs for each TF (Methods). As expected, PWMs derived from visualizingfilters associated with important units performed better: for 139 TFs (85.3%), the bestperforming PWM was derived from the filter of one of the 10 most important units (Fig. 2A).Next, we applied STREME31, a state-of-the-art method for de novo motif discovery, on thesame sequences used for training the ExplaiNN models to discover 100 motifs for each TF(Methods). Pairwise comparison of the performances of the best PWMs obtained by eachmethod revealed subtle differences (Fig. 2B), although PWMs discovered by STREME weresuperior for TFs with small training datasets (Fig. 2C). Notably, the execution times exhibited7made available under aCC-BY-NC-ND 4.0 International license.(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It isbioRxiv preprint doi: https://doi.org/10.1101/2022.05.20.492818; this version posted May 22, 2022. The copyright holder for this preprintby the two methods differed greatly, with ExplaiNN being >100 times faster for TFs with largetraining datasets of ≥50,000 sequences (Fig. 2D). These differences were consistent with aprevious report related to the benefits of GPU-enabled de novo motif discovery30.Motivated by the success of ExplaiNN in discovering motifs in in vivo data and todemonstrate its capacity on data from various assays, we benchmarked ExplaiNN againstother de novo methods, but this time using in vitro data (Methods). We downloaded publiclyavailable HT-SELEX34, PBM35, and SMiLE-seq36 data for the TF GATA3, as well as thePWMs discovered in these datasets by three assay-specific de novo motif discoverymethods: Autoseed37 (HT-SELEX), Seed-and-Wobble38 (PBM), and a hidden Markov model(HMM) approach (SMiLE-seq). The performance of the best PWMs derived with ExplaiNNwas consistent regardless of the type of assay (Fig. 2E), as were their logos (Fig. 2F).Focusing on specific assays, the capacity of ExplaiNN to discover de novo motifs in the invivo and SMiLE-seq data, as measured by the performance of the best PWMs derived, wascomparable to that of STREME and the HMM-based method, respectively, whileoutperforming Autoseed and Seed-and-Wobble in their corresponding assays (Fig. 2E). AllGATA3 logos were very similar to each other, and were also similar to the logo from JASPARfor this TF profile (MA0037.4), derived originally by applying RSAT32 on a mouse Gata3ChIP-seq data from ReMap39 (Fig. 2F). Taken together, this supports the potential ofExplaiNN as a fast, universal method for de novo motif discovery.ExplaiNN recapitulates the cis-regulatory lexicon of immune cell differentiationTo further explore the capabilities of ExplaiNN on distinct data types, we compared itsperformance against AI-TAC in predicting chromatin accessibilities in 81 immune cell typesfrom 6 different lineages9. We started with an exploratory analysis to determine the optimalnumber of units to train ExplaiNN. Saturation in model performance by means of averagePCC between predicted and actual ATAC signals, as well as in the number of well-predictedsequences, occurred at ~250 units (Fig. 3A; Methods), however, we decided to use 300units, which is the same number of convolutional filters used in the first layer of AI-TAC. Theperformance of ExplaiNN by means of average PCC was comparable to that of AI-TAC(1-2% difference; Fig. 3A), and the PCCs of individual sequences correlated well betweenthe two models (Fig. 3B), however, AI-TAC correctly predicted more sequences (Fig. 3A).Next, we visualized both the filters and weights of each unit of the ExplaiNN model,identifying the same lineage-specific TF motifs reported in AI-TAC without having to undergothe computationally intensive process of filter nullification (Fig. 3C): NFE2, NFI, and GATA(in stem cells); POU, EBF, and PAX (in B cells); TCF3, TCF7, Ets, and AP1 (in T cells); NR18made available under aCC-BY-NC-ND 4.0 International license.(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It isbioRxiv preprint doi: https://doi.org/10.1101/2022.05.20.492818; this version posted May 22, 2022. The copyright holder for this preprint(nuclear receptor type 1), TBX, and REL (in innate lymphoid cells); and SPI, Krüppel zincfingers, and CEBP (in myeloid cells). Moreover, we visualized the importances of each unitto understand their influence on the model's predictions. For example, CEBP- and PAX-likeunits were important for predicting accessibility in most cell types of the myeloid and Blineages, respectively (Figs. 3D and E). Taken together, using ExplaiNN, we replicated theresults of AI-TAC without the need to apply complex and time-consuming interpretationtechniques by simply visualizing the weights and importances of each unit.ExplaiNN is suitable for the analysis of single-cell chromatin accessibility dataSingle-cell (sc) sequencing methods enable profiling of a wide range of genomic informationin individual cells (reviewed in 40), including chromatin accessibility. To explore the utility ofExplaiNN for deciphering cis-regulatory properties from granular sc data, we reanalyzed arecent scATAC-seq dataset cataloging 228,873 OCRs across 15,298 human pancreatic isletcells that were grouped into 12 clusters based on their accessibility profiles41. We trained aExplaiNN model with 400 units (i.e. the number of units at which model performanceplateaued) on the sc data to predict the activity of each OCR across the 12 clusters, andvisualized both the filters and weights of each unit. We observed that the weights of someunits exhibited cell type-specific patterns that had also been found using chromVAR42 in theoriginal study (Fig. 4A). For instance, PDX-like units had high positive weights for beta anddelta cells, MAF-like units for alpha and beta cells, HNF1-like units for alpha and gamma butnot for ductal cells, and FOX-like units for alpha, beta and gamma cells. However, therewere also differences: some units did not exhibit high positive weights in expected cell types(e.g. Ets-like units had negative weights in endothelial cells), while others exhibited celltype-specific patterns not reported in the original study (Fig. 4A). Next, we visualized theimportances of each unit for their contributions to the physiological stratification of the cells,finding that RFX-like units were important for predicting OCRs in hormone-high (i.e. alpha,beta, and delta type 1 cells) but not in hormone-low cells (i.e. alpha, beta, and delta type 2cells) (Fig. 4B). It was the opposite for AP1-like units: they were important for hormone-lowbut not hormone-high cells (Fig. 4C). Taken together, ExplaiNN was able to reproduce andexpand on the results from the original study while demonstrating its utility for the analysisand interpretation of scATAC-seq data.ExplaiNN as a plug-and-play platform for TF motifs and deep learning modelsIn ExplaiNN, a key step during model interpretation is annotating each unit to ease biologicalinterpretation. Given that ExplaiNN models can be conceptualized as a PWM scanning layerfeeding into fully connected layers, we reasoned that initializing the weights of each unit filter9made available under aCC-BY-NC-ND 4.0 International license.(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It isbioRxiv preprint doi: https://doi.org/10.1101/2022.05.20.492818; this version posted May 22, 2022. The copyright holder for this preprintwith a JASPAR profile would facilitate the interpretation process because the biologicalannotations of the units would be known beforehand. To confirm this, we trained ExplaiNNmodels with increasing numbers of units (from 300 to 1,492) on the AI-TAC dataset in whichthe filter weights of each unit had been initialized with JASPAR profiles (Fig. 5A; Methods).During training, the filter weights were frozen to prevent them from being refined (i.e. themodels were only allowed to learn the weights of the fully connected and final linear layers).These JASPAR-initialized, frozen models, even the largest attempt with 1,492 units,performed much worse than both AI-TAC and an ExplaiNN model with 300 units trained fromscratch (Fig. 5B). Still, the importances of some units were informative. For example, a unitwhose filter weights had been initialized with the profile of Lef1 (MA0768.2) was importantfor predicting accessibility in T cells (Fig. S3), in agreement with the role of this TF inestablishing T cell identity43. We attributed the overall poor performance of these frozenmodels to the fact that many JASPAR profiles used to initialize the filters might be from TFsirrelevant to immune cells and, therefore, a refinement process would be required to allowthem to better resemble the motifs of relevant TFs. Indeed, unfreezing the filter weightsduring training improved the performance of the model, approaching that of AI-TAC (Fig.5B). Allowing refinement resulted in >35% of the filters undergoing substantial changes; theirvisualization as PWMs revealed that they had become different from the original JASPARprofiles used for their initialization (Tomtom24 q-value >0.05). For example, a unit whose filterweights had been initialized with the profile of TFAP2C (MA0815.1), and whose importanceacross the different immune lineages when freezing the filter weights during training wasnull, became important for predicting accessibility in B cells. Its filter was refined to such anextent that when visualized as a PWM it resembled the motif of EBF1, an important TF formaintaining B cell identity44 (Fig. 5C).A limitation of the JASPAR-initialization-and-freezing approach was the inability todistinguish between TFs from the same family (i.e. TFs sharing the same class ofDNA-binding domain), as they often have highly similar DNA-binding specificities45. In orderto generate individual units that provide greater resolution, we implemented a transferlearning strategy (Fig. 5D; Methods): We pre-trained 350 single-task DanQ models, eachpredicting the binding of a single TF to the mouse genome, and initialized an ExplaiNNmodel with 350 units in which we replaced the layers of each unit with one of the pre-trainedDanQ models. Akin to NAMs, we initialized a second model in which we added two fullyconnected layers after the DanQ models. The AUPRCs of the pre-trained DanQ modelsranged from 0.51 (E4f1) to 0.96 (Snai2), with a median of 0.78 across all models (Table S2).Then, we fine-tuned both ExplaiNN models on the AI-TAC dataset, freezing the pre-trainedDanQ models (i.e. their weights were not modified). The performance of the fine-tuned10made available under aCC-BY-NC-ND 4.0 International license.(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It isbioRxiv preprint doi: https://doi.org/10.1101/2022.05.20.492818; this version posted May 22, 2022. The copyright holder for this preprintmodels almost reached that of AI-TAC (PCC = 0.341 vs. 0.345 for AI-TAC). Next, wevisualized the importances of each DanQ model unit in the ExplaiNN model without fullyconnected layers, allowing us to disambiguate the contribution of individual TF familymembers (Methods). For instance, the Irf4 and Irf8 units were important for predictingaccessibility in different immune lineages (Fig. 5E), in agreement with the distinct roles ofthese TFs: Irf4 regulates B, T, myeloid, and dendritic cell differentiation46, while Irf8 regulatesB and myeloid cell lineages47. Similarly, the Pax5 unit was important for predictingaccessibility in B cells, consistent with the role of this TF in establishing B lineage identityand function48; in contrast, the importances of the Pax3 and Pax7 units across immunelineages were negligible, in agreement with their role in regulating myogenesis49 (Fig. 5F).Finally, we applied UMAP50 to cluster the sequences based on their unit outputs in thesecond ExplaiNN model, resulting in three main clusters that were associated with ATACsignals in alpha/beta T, myeloid, and B cells (Figs. 5G and S4). The outputs of some unitswere in strong agreement with a biologically relevant cluster. For example, the Bcl11b unitoutputs were confined within the boundaries of the alpha/beta T cell cluster, consistent withits role in the differentiation and survival of these lymphocytes51, the Cebpa unit outputswithin the myeloid cluster, in agreement with its role in myeloid differentiation52, and the Ebf1unit outputs within the B cluster (Figs. 5G and S4). Taken together, replacing the units ofExplaiNN with pre-trained high-resolution TF binding models achieved performance levelscomparable to state-of-the-art deep learning models while providing the additional value ofgaining insights into the roles of individual TFs. This flexibility of incorporating differentcomponents into ExplaiNN offers the potential for increased performance while retaininginterpretability."))