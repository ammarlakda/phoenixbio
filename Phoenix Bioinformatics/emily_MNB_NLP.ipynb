{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/emilyjiang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/emilyjiang/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/emilyjiang/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     /Users/emilyjiang/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/emilyjiang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/emilyjiang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/emilyjiang/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import important modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# sklearn modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB # classifier \n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    plot_confusion_matrix,\n",
    ")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# text preprocessing modules\n",
    "from string import punctuation \n",
    "# text preprocessing modules\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "# adding two more downloads based on warnings generated when cleaning text \n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re #regular expression\n",
    "# Download dependency\n",
    "for dependency in (\n",
    "    \"brown\",\n",
    "    \"names\",\n",
    "    \"wordnet\",\n",
    "    \"averaged_perceptron_tagger\",\n",
    "    \"universal_tagset\",\n",
    "):\n",
    "    nltk.download(dependency)\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# seeding\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/6q/ndm97whn24zdlr8rtgm_lr1w0000gn/T/ipykernel_94573/4074912119.py\", line 2, in <cell line: 2>\n",
      "    import tensorflow as tf\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/__init__.py\", line 37, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/__init__.py\", line 42, in <module>\n",
      "    from tensorflow.python import data\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/__init__.py\", line 21, in <module>\n",
      "    from tensorflow.python.data import experimental\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/experimental/__init__.py\", line 96, in <module>\n",
      "    from tensorflow.python.data.experimental import service\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/experimental/service/__init__.py\", line 419, in <module>\n",
      "    from tensorflow.python.data.experimental.ops.data_service_ops import distribute\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/data_service_ops.py\", line 22, in <module>\n",
      "    from tensorflow.python.data.experimental.ops import compression_ops\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/compression_ops.py\", line 16, in <module>\n",
      "    from tensorflow.python.data.util import structure\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py\", line 22, in <module>\n",
      "    from tensorflow.python.data.util import nest\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/util/nest.py\", line 34, in <module>\n",
      "    from tensorflow.python.framework import sparse_tensor as _sparse_tensor\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/sparse_tensor.py\", line 24, in <module>\n",
      "    from tensorflow.python.framework import constant_op\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 25, in <module>\n",
      "    from tensorflow.python.eager import execute\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\", line 21, in <module>\n",
      "    from tensorflow.python.framework import dtypes\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py\", line 34, in <module>\n",
      "    _np_bfloat16 = _pywrap_bfloat16.TF_bfloat16_type()\n",
      "TypeError: Unable to convert function return value to a Python type! The signature was\n",
      "\t() -> handle\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1993, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow and keras for padding \n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/6q/ndm97whn24zdlr8rtgm_lr1w0000gn/T/ipykernel_94573/643355293.py\", line 1, in <cell line: 1>\n",
      "    import keras as k\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/keras/__init__.py\", line 21, in <module>\n",
      "    from keras import models\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/keras/models/__init__.py\", line 18, in <module>\n",
      "    from keras.engine.functional import Functional\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/keras/engine/functional.py\", line 24, in <module>\n",
      "    import tensorflow.compat.v2 as tf\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/__init__.py\", line 37, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/__init__.py\", line 42, in <module>\n",
      "    from tensorflow.python import data\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/__init__.py\", line 21, in <module>\n",
      "    from tensorflow.python.data import experimental\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/experimental/__init__.py\", line 96, in <module>\n",
      "    from tensorflow.python.data.experimental import service\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/experimental/service/__init__.py\", line 419, in <module>\n",
      "    from tensorflow.python.data.experimental.ops.data_service_ops import distribute\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/data_service_ops.py\", line 22, in <module>\n",
      "    from tensorflow.python.data.experimental.ops import compression_ops\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/compression_ops.py\", line 16, in <module>\n",
      "    from tensorflow.python.data.util import structure\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py\", line 22, in <module>\n",
      "    from tensorflow.python.data.util import nest\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/util/nest.py\", line 34, in <module>\n",
      "    from tensorflow.python.framework import sparse_tensor as _sparse_tensor\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/sparse_tensor.py\", line 24, in <module>\n",
      "    from tensorflow.python.framework import constant_op\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 25, in <module>\n",
      "    from tensorflow.python.eager import execute\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\", line 21, in <module>\n",
      "    from tensorflow.python.framework import dtypes\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py\", line 34, in <module>\n",
      "    _np_bfloat16 = _pywrap_bfloat16.TF_bfloat16_type()\n",
      "TypeError: Unable to convert function return value to a Python type! The signature was\n",
      "\t() -> handle\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1993, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/emilyjiang/opt/anaconda3/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import keras as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>ArticleTitle</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>volume</th>\n",
       "      <th>date</th>\n",
       "      <th>pubmed</th>\n",
       "      <th>doi_pii_str</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B. Jiang, Y. Shi, Y. Peng, Y. Jia, Y. Yan, X. ...</td>\n",
       "      <td>\"Cold-Induced CBF-PIF3 Interaction Enhances Fr...</td>\n",
       "      <td>Molecular plant</td>\n",
       "      <td>13(6)</td>\n",
       "      <td>(Jun. 2020).</td>\n",
       "      <td>PUBMED: 32311530;</td>\n",
       "      <td>DOI 10.1016/j.molp.2020.04.006.</td>\n",
       "      <td>Growth inhibition and cold-acclimation strateg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Z. Mao, S. He, F. Xu, X. Wei, L. Jiang, Y. Liu...</td>\n",
       "      <td>\"Photoexcited CRY1 and phyB interact directly ...</td>\n",
       "      <td>The New phytologist</td>\n",
       "      <td>225(2)</td>\n",
       "      <td>(Jan. 2020).</td>\n",
       "      <td>PUBMED: 31514232;</td>\n",
       "      <td>DOI 10.1111/nph.16194.</td>\n",
       "      <td>Arabidopsis CRY1 and phyB are the primary blue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F. Zheng, Y. Wang, D. Gu and X. Liu,</td>\n",
       "      <td>\"Histone Deacetylase HDA15 Restrains PHYB-Depe...</td>\n",
       "      <td>Cells</td>\n",
       "      <td>11(23)</td>\n",
       "      <td>(Nov. 2022).</td>\n",
       "      <td>PUBMED: 36497048;</td>\n",
       "      <td>DOI 10.3390/cells11233788.</td>\n",
       "      <td>Seed germination is essential for the coloniza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JH. Jung, M. Domijan, C. Klose, S. Biswas, D. ...</td>\n",
       "      <td>\"Phytochromes function as thermosensors in Ara...</td>\n",
       "      <td>Science (New York, N.Y.)</td>\n",
       "      <td>354(6314)</td>\n",
       "      <td>(Nov. 2016).</td>\n",
       "      <td>PUBMED: 27789797;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plants are responsive to temperature, and some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T. Yan, Y. Heng, W. Wang, J. Li and XW. Deng,</td>\n",
       "      <td>\"SWELLMAP 2, a phyB-Interacting Splicing Facto...</td>\n",
       "      <td>Frontiers in plant science</td>\n",
       "      <td>13</td>\n",
       "      <td>(2022).</td>\n",
       "      <td>PUBMED: 35222493;</td>\n",
       "      <td>DOI 10.3389/fpls.2022.836519.</td>\n",
       "      <td>Light-triggered transcriptome reprogramming is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             authors  \\\n",
       "0  B. Jiang, Y. Shi, Y. Peng, Y. Jia, Y. Yan, X. ...   \n",
       "1  Z. Mao, S. He, F. Xu, X. Wei, L. Jiang, Y. Liu...   \n",
       "2              F. Zheng, Y. Wang, D. Gu and X. Liu,    \n",
       "3  JH. Jung, M. Domijan, C. Klose, S. Biswas, D. ...   \n",
       "4     T. Yan, Y. Heng, W. Wang, J. Li and XW. Deng,    \n",
       "\n",
       "                                        ArticleTitle  \\\n",
       "0  \"Cold-Induced CBF-PIF3 Interaction Enhances Fr...   \n",
       "1  \"Photoexcited CRY1 and phyB interact directly ...   \n",
       "2  \"Histone Deacetylase HDA15 Restrains PHYB-Depe...   \n",
       "3  \"Phytochromes function as thermosensors in Ara...   \n",
       "4  \"SWELLMAP 2, a phyB-Interacting Splicing Facto...   \n",
       "\n",
       "                 journal_title      volume           date              pubmed  \\\n",
       "0             Molecular plant       13(6)   (Jun. 2020).   PUBMED: 32311530;    \n",
       "1         The New phytologist      225(2)   (Jan. 2020).   PUBMED: 31514232;    \n",
       "2                       Cells      11(23)   (Nov. 2022).   PUBMED: 36497048;    \n",
       "3    Science (New York, N.Y.)   354(6314)   (Nov. 2016).   PUBMED: 27789797;    \n",
       "4  Frontiers in plant science          13        (2022).   PUBMED: 35222493;    \n",
       "\n",
       "                       doi_pii_str  \\\n",
       "0  DOI 10.1016/j.molp.2020.04.006.   \n",
       "1           DOI 10.1111/nph.16194.   \n",
       "2       DOI 10.3390/cells11233788.   \n",
       "3                              NaN   \n",
       "4    DOI 10.3389/fpls.2022.836519.   \n",
       "\n",
       "                                            abstract  \n",
       "0  Growth inhibition and cold-acclimation strateg...  \n",
       "1  Arabidopsis CRY1 and phyB are the primary blue...  \n",
       "2  Seed germination is essential for the coloniza...  \n",
       "3  Plants are responsive to temperature, and some...  \n",
       "4  Light-triggered transcriptome reprogramming is...  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from phyb_400 csv \n",
    "df_original = pd.read_csv(\"/Users/emilyjiang/Desktop/QMIND_Ciaran_Webscraping/PHYB_400.csv\", index_col=0) \n",
    "\n",
    "# print beginning of data to check \n",
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 8)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking shape of dataset \n",
    "df_original.shape\n",
    "# dataset has 8 columns, 400 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "authors            1\n",
       "ArticleTitle       0\n",
       "journal_title      0\n",
       "volume             7\n",
       "date               0\n",
       "pubmed             0\n",
       "doi_pii_str      128\n",
       "abstract           5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if  dataset has any missing values\n",
    "df_original.isnull().sum()\n",
    "\n",
    "# check what to do with missing values? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ciaran's next code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# import dataset\\ndf_orig = pd.read_csv(\"PHYB_400.csv\", index_col=0)\\n# download a list of useless stop words\\ndf_orig.iloc[:,-1] = df_orig.iloc[:,-1].astype(str)\\nstop_words = set(stopwords.words(\\'english\\')) \\n# nltk.download(\\'stopwords\\')\\n# print(nltk.download(\\'stopwords\\'))\\n# print(filter(lambda w: not w in s, df.iloc[1,-1]))\\n# stop_words\\ndf_orig\\n\\n# tokenize the words (make the paragraph into a list of strings for each word)\\ndf = df_orig\\n\\nfor i in tqdm(range(len(df))):\\n    df.iloc[i,-1] = word_tokenize(df.iloc[i,-1])\\n    # df.iloc[i,-1] = \"\".join([word + \" \" for word in text_tokens if not word in stopwords.words()])\\n# convert to the lsit to a string\\ndf.iloc[:,-1] = df.iloc[:,-1].astype(str)\\n# strip bad characters\\nbad_letters = [\\'\\'\\', \\'.\\', \\',\\', \\'[\\', \\']\\', \\'(\\', \\')\\']\\nfor i in bad_letters:\\n    df.iloc[:,-1] = df.iloc[:,-1].map(lambda x: x.replace(i,\\'\\'))\\n# remove multiple spaces\\nfor i in range(5):  \\n    df.iloc[:,-1] = df.iloc[:,-1].map(lambda x: x.replace(\\'  \\',\\' \\'))\\nprint(df.iloc[0,-1])\\n\\n#Tokenize the sentences (but using keras this time)\\nmyTokenizer = Tokenizer(num_words=1000)\\nmyTokenizer.fit_on_texts(df.iloc[:,-1])\\nsequences = myTokenizer.texts_to_sequences(df.iloc[:,-1])\\n\\n# Padding ( make everything the same dimension by adding zeros to the front)\\npadded = pad_sequences(sequences, maxlen=len(df.iloc[:,-1][3].split(\" \")))\\n\\ndf[\\'Count\\'] = df[\\'abstract\\'].map(lambda x: x.count(\"phyB\"))\\n# df[df[\\'Count\\'] > 0]\\ndf[\\'Label\\'] = sum([df[\\'Count\\'] > 3])'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# import dataset\n",
    "df_orig = pd.read_csv(\"PHYB_400.csv\", index_col=0)\n",
    "# download a list of useless stop words\n",
    "df_orig.iloc[:,-1] = df_orig.iloc[:,-1].astype(str)\n",
    "stop_words = set(stopwords.words('english')) \n",
    "# nltk.download('stopwords')\n",
    "# print(nltk.download('stopwords'))\n",
    "# print(filter(lambda w: not w in s, df.iloc[1,-1]))\n",
    "# stop_words\n",
    "df_orig\n",
    "\n",
    "# tokenize the words (make the paragraph into a list of strings for each word)\n",
    "df = df_orig\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    df.iloc[i,-1] = word_tokenize(df.iloc[i,-1])\n",
    "    # df.iloc[i,-1] = \"\".join([word + \" \" for word in text_tokens if not word in stopwords.words()])\n",
    "# convert to the lsit to a string\n",
    "df.iloc[:,-1] = df.iloc[:,-1].astype(str)\n",
    "# strip bad characters\n",
    "bad_letters = ['\\'', '.', ',', '[', ']', '(', ')']\n",
    "for i in bad_letters:\n",
    "    df.iloc[:,-1] = df.iloc[:,-1].map(lambda x: x.replace(i,''))\n",
    "# remove multiple spaces\n",
    "for i in range(5):  \n",
    "    df.iloc[:,-1] = df.iloc[:,-1].map(lambda x: x.replace('  ',' '))\n",
    "print(df.iloc[0,-1])\n",
    "\n",
    "#Tokenize the sentences (but using keras this time)\n",
    "myTokenizer = Tokenizer(num_words=1000)\n",
    "myTokenizer.fit_on_texts(df.iloc[:,-1])\n",
    "sequences = myTokenizer.texts_to_sequences(df.iloc[:,-1])\n",
    "\n",
    "# Padding ( make everything the same dimension by adding zeros to the front)\n",
    "padded = pad_sequences(sequences, maxlen=len(df.iloc[:,-1][3].split(\" \")))\n",
    "\n",
    "df['Count'] = df['abstract'].map(lambda x: x.count(\"phyB\"))\n",
    "# df[df['Count'] > 0]\n",
    "df['Label'] = sum([df['Count'] > 3])'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking line to read abstract of article from Ciaran's model (***CHECK THAT THAT LINE READS ABSTRACT AS STRING***): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Growth inhibition and cold-acclimation strateg...\n",
      "1      Arabidopsis CRY1 and phyB are the primary blue...\n",
      "2      Seed germination is essential for the coloniza...\n",
      "3      Plants are responsive to temperature, and some...\n",
      "4      Light-triggered transcriptome reprogramming is...\n",
      "                             ...                        \n",
      "395    The phytochrome family of red/far-red (R/FR)-r...\n",
      "396    Phytochromes are red (R) and far-red (FR) ligh...\n",
      "397    The phytochrome (phy) family of sensory photor...\n",
      "398    Light is a crucial environmental signal that c...\n",
      "399    Overexpression of phytochrome B (phyB) in Arab...\n",
      "Name: abstract, Length: 400, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_original.iloc[:,-1] = df_original.iloc[:,-1].astype(str)\n",
    "print(df_original.iloc[:,-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using text_cleaning() function to clean dataset of text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words =  stopwords.words('english')\n",
    "def text_cleaning(text, remove_stop_words=True, lemmatize_words=True):\n",
    "    # Clean the text, with the option to remove stop_words and to lemmatize word\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text =  re.sub(r'http\\S+',' link ', text)\n",
    "    text = re.sub(r'\\b\\d+(?:\\.\\d+)?\\s+', '', text) # remove numbers\n",
    "        \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stop_words:\n",
    "        text = text.split()\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if lemmatize_words:\n",
    "        text = text.split()\n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in text]\n",
    "        text = \" \".join(lemmatized_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use text_cleaning() function to clean data, accessing last column of dataframe (which is the abstract)\n",
    "Creates a new column for cleaned data. Using text_cleaning function from above chunk of code, we are cleaning the text in the abstract column and saving this cleaned text in a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>ArticleTitle</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>volume</th>\n",
       "      <th>date</th>\n",
       "      <th>pubmed</th>\n",
       "      <th>doi_pii_str</th>\n",
       "      <th>abstract</th>\n",
       "      <th>cleaned_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B. Jiang, Y. Shi, Y. Peng, Y. Jia, Y. Yan, X. ...</td>\n",
       "      <td>\"Cold-Induced CBF-PIF3 Interaction Enhances Fr...</td>\n",
       "      <td>Molecular plant</td>\n",
       "      <td>13(6)</td>\n",
       "      <td>(Jun. 2020).</td>\n",
       "      <td>PUBMED: 32311530;</td>\n",
       "      <td>DOI 10.1016/j.molp.2020.04.006.</td>\n",
       "      <td>Growth inhibition and cold-acclimation strateg...</td>\n",
       "      <td>Growth inhibition and cold-acclimation strateg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Z. Mao, S. He, F. Xu, X. Wei, L. Jiang, Y. Liu...</td>\n",
       "      <td>\"Photoexcited CRY1 and phyB interact directly ...</td>\n",
       "      <td>The New phytologist</td>\n",
       "      <td>225(2)</td>\n",
       "      <td>(Jan. 2020).</td>\n",
       "      <td>PUBMED: 31514232;</td>\n",
       "      <td>DOI 10.1111/nph.16194.</td>\n",
       "      <td>Arabidopsis CRY1 and phyB are the primary blue...</td>\n",
       "      <td>Arabidopsis CRY1 and phyB are the primary blue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F. Zheng, Y. Wang, D. Gu and X. Liu,</td>\n",
       "      <td>\"Histone Deacetylase HDA15 Restrains PHYB-Depe...</td>\n",
       "      <td>Cells</td>\n",
       "      <td>11(23)</td>\n",
       "      <td>(Nov. 2022).</td>\n",
       "      <td>PUBMED: 36497048;</td>\n",
       "      <td>DOI 10.3390/cells11233788.</td>\n",
       "      <td>Seed germination is essential for the coloniza...</td>\n",
       "      <td>Seed germination is essential for the coloniza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JH. Jung, M. Domijan, C. Klose, S. Biswas, D. ...</td>\n",
       "      <td>\"Phytochromes function as thermosensors in Ara...</td>\n",
       "      <td>Science (New York, N.Y.)</td>\n",
       "      <td>354(6314)</td>\n",
       "      <td>(Nov. 2016).</td>\n",
       "      <td>PUBMED: 27789797;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plants are responsive to temperature, and some...</td>\n",
       "      <td>Plants are responsive to temperature, and some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T. Yan, Y. Heng, W. Wang, J. Li and XW. Deng,</td>\n",
       "      <td>\"SWELLMAP 2, a phyB-Interacting Splicing Facto...</td>\n",
       "      <td>Frontiers in plant science</td>\n",
       "      <td>13</td>\n",
       "      <td>(2022).</td>\n",
       "      <td>PUBMED: 35222493;</td>\n",
       "      <td>DOI 10.3389/fpls.2022.836519.</td>\n",
       "      <td>Light-triggered transcriptome reprogramming is...</td>\n",
       "      <td>Light-triggered transcriptome reprogramming is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             authors  \\\n",
       "0  B. Jiang, Y. Shi, Y. Peng, Y. Jia, Y. Yan, X. ...   \n",
       "1  Z. Mao, S. He, F. Xu, X. Wei, L. Jiang, Y. Liu...   \n",
       "2              F. Zheng, Y. Wang, D. Gu and X. Liu,    \n",
       "3  JH. Jung, M. Domijan, C. Klose, S. Biswas, D. ...   \n",
       "4     T. Yan, Y. Heng, W. Wang, J. Li and XW. Deng,    \n",
       "\n",
       "                                        ArticleTitle  \\\n",
       "0  \"Cold-Induced CBF-PIF3 Interaction Enhances Fr...   \n",
       "1  \"Photoexcited CRY1 and phyB interact directly ...   \n",
       "2  \"Histone Deacetylase HDA15 Restrains PHYB-Depe...   \n",
       "3  \"Phytochromes function as thermosensors in Ara...   \n",
       "4  \"SWELLMAP 2, a phyB-Interacting Splicing Facto...   \n",
       "\n",
       "                 journal_title      volume           date              pubmed  \\\n",
       "0             Molecular plant       13(6)   (Jun. 2020).   PUBMED: 32311530;    \n",
       "1         The New phytologist      225(2)   (Jan. 2020).   PUBMED: 31514232;    \n",
       "2                       Cells      11(23)   (Nov. 2022).   PUBMED: 36497048;    \n",
       "3    Science (New York, N.Y.)   354(6314)   (Nov. 2016).   PUBMED: 27789797;    \n",
       "4  Frontiers in plant science          13        (2022).   PUBMED: 35222493;    \n",
       "\n",
       "                       doi_pii_str  \\\n",
       "0  DOI 10.1016/j.molp.2020.04.006.   \n",
       "1           DOI 10.1111/nph.16194.   \n",
       "2       DOI 10.3390/cells11233788.   \n",
       "3                              NaN   \n",
       "4    DOI 10.3389/fpls.2022.836519.   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Growth inhibition and cold-acclimation strateg...   \n",
       "1  Arabidopsis CRY1 and phyB are the primary blue...   \n",
       "2  Seed germination is essential for the coloniza...   \n",
       "3  Plants are responsive to temperature, and some...   \n",
       "4  Light-triggered transcriptome reprogramming is...   \n",
       "\n",
       "                                        cleaned_data  \n",
       "0  Growth inhibition and cold-acclimation strateg...  \n",
       "1  Arabidopsis CRY1 and phyB are the primary blue...  \n",
       "2  Seed germination is essential for the coloniza...  \n",
       "3  Plants are responsive to temperature, and some...  \n",
       "4  Light-triggered transcriptome reprogramming is...  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_original[\"cleaned_data\"] = df_original[df_original.columns[len(df_original.columns)-1]]\n",
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTokenizer = Tokenizer(num_words=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing data, padding data, counting occurences of \"phyB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the sentences (but using keras this time)\n",
    "myTokenizer.fit_on_texts(df_original.iloc[:,-1])\n",
    "sequences = myTokenizer.texts_to_sequences(df_original.iloc[:,-1])\n",
    "\n",
    "# padding (make everything the same dimension by adding zeros to the front)\n",
    "padded = pad_sequences(sequences, maxlen=len(df_original.iloc[:,-1][3].split(\" \")))\n",
    "\n",
    "# count number of \"phyB\" occurrences in cleaned text column \n",
    "df_original['count'] = df_original[df_original.columns[len(df_original.columns)-1]].map(lambda x: x.count(\"phyB\"))\n",
    "# df_original[df_original['count'] > 0]\n",
    "df_original['label'] = sum([df_original['count'] > 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      1\n",
      "1      1\n",
      "2      0\n",
      "3      0\n",
      "4      1\n",
      "      ..\n",
      "395    0\n",
      "396    0\n",
      "397    1\n",
      "398    0\n",
      "399    1\n",
      "Name: label, Length: 400, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_original['label']) \n",
    "sum(df_original['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_original[\"cleaned_data\"] \n",
    "y = df_original.label.values "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into train and test data, test = 15% of dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, \n",
    "test_size=0.30,\n",
    "random_state=42, \n",
    "shuffle=True,\n",
    "stratify=y,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating NLP Model: Now training Multinomial Naive Bayes algorithm (common algorithm for text classification) to classify if article is about phyB or not"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform cleaned data into numerical values so model can understand it. Using TfidfVectorizer method from scikit-learn (converts text documents to matrix of TF-IDF features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Pipeline class from scikit-learn to apply list of transforms and final estimator for pre-processing and training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "phyB_classifier = Pipeline(steps=[\n",
    "    ('pre_processing',TfidfVectorizer(lowercase=False)),\n",
    "    ('naive_bayes',MultinomialNB())\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;pre_processing&#x27;, TfidfVectorizer(lowercase=False)),\n",
       "                (&#x27;naive_bayes&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;pre_processing&#x27;, TfidfVectorizer(lowercase=False)),\n",
       "                (&#x27;naive_bayes&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(lowercase=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('pre_processing', TfidfVectorizer(lowercase=False)),\n",
       "                ('naive_bayes', MultinomialNB())])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phyB_classifier.fit(x_train, y_train) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictiom from validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = phyB_classifier.predict(x_valid) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6833333333333333"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, y_predict) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/emilyjiang/Desktop/Webscraping']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# import it to the webscraping folder\n",
    "joblib.dump(phyB_classifier, '/Users/emilyjiang/Desktop/Webscraping')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdec36b3ce009a3c34851a12eb9b6c73eae6306fd6680792593ee6082769bc5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
