{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/emilyjiang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/emilyjiang/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/emilyjiang/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     /Users/emilyjiang/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/emilyjiang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/emilyjiang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/emilyjiang/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import important modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# sklearn modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB # classifier \n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    plot_confusion_matrix,\n",
    ")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# text preprocessing modules\n",
    "from string import punctuation \n",
    "# text preprocessing modules\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "# adding two more downloads based on warnings generated when cleaning text \n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re #regular expression\n",
    "# Download dependency\n",
    "for dependency in (\n",
    "    \"brown\",\n",
    "    \"names\",\n",
    "    \"wordnet\",\n",
    "    \"averaged_perceptron_tagger\",\n",
    "    \"universal_tagset\",\n",
    "):\n",
    "    nltk.download(dependency)\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# seeding\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unable to convert function return value to a Python type! The signature was\n\t() -> handle",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/emilyjiang/Desktop/Git/Phoenix Bioinformatics/emily_MNB_NLP.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emilyjiang/Desktop/Git/Phoenix%20Bioinformatics/emily_MNB_NLP.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# import tensorflow and keras for padding \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/emilyjiang/Desktop/Git/Phoenix%20Bioinformatics/emily_MNB_NLP.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emilyjiang/Desktop/Git/Phoenix%20Bioinformatics/emily_MNB_NLP.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mk\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/__init__.py:42\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n\u001b[1;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[1;32m     44\u001b[0m \u001b[39m# from tensorflow.python import keras\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/__init__.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"`tf.data.Dataset` API for input pipelines.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m experimental\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m AUTOTUNE\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/experimental/__init__.py:95\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Experimental API for building input pipelines.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39mThis module contains experimental `Dataset` sources and transformations that can\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39m@@UNKNOWN_CARDINALITY\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m \u001b[39mimport\u001b[39;00m service\n\u001b[1;32m     96\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatching\u001b[39;00m \u001b[39mimport\u001b[39;00m dense_to_ragged_batch\n\u001b[1;32m     97\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatching\u001b[39;00m \u001b[39mimport\u001b[39;00m dense_to_sparse_batch\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/experimental/service/__init__.py:387\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"API for using the tf.data service.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39mThis module contains:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39m  job of ParameterServerStrategy).\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_service_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[1;32m    388\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_service_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m from_dataset_id\n\u001b[1;32m    389\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_service_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m register_dataset\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/data_service_ops.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m data_service_pb2\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m tf2\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m compression_ops\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mservice\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_server_lib\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mservice\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_utils\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/compression_ops.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m structure\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m gen_experimental_dataset_ops \u001b[39mas\u001b[39;00m ged_ops\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompress\u001b[39m(element):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msix\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwrapt\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m nest\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m composite_tensor\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m ops\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/util/nest.py:36\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m\"\"\"## Functions for working with arbitrarily nested sequences of elements.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[39mNOTE(mrry): This fork of the `tensorflow.python.util.nest` module\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m   arrays.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msix\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_six\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m sparse_tensor \u001b[39mas\u001b[39;00m _sparse_tensor\n\u001b[1;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_utils\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m nest\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/sparse_tensor.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m tf2\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m composite_tensor\n\u001b[0;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m constant_op\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m dtypes\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m ops\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:25\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m types_pb2\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m execute\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m dtypes\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m op_callbacks\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tfe\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m core\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m dtypes\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m ops\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:29\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_bfloat16\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtf_export\u001b[39;00m \u001b[39mimport\u001b[39;00m tf_export\n\u001b[0;32m---> 29\u001b[0m _np_bfloat16 \u001b[39m=\u001b[39m _pywrap_bfloat16\u001b[39m.\u001b[39;49mTF_bfloat16_type()\n\u001b[1;32m     32\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdtypes.DType\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDType\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDType\u001b[39;00m(_dtypes\u001b[39m.\u001b[39mDType):\n\u001b[1;32m     34\u001b[0m   \u001b[39m\"\"\"Represents the type of the elements in a `Tensor`.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[39m  `DType`'s are used to specify the output data type for operations which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m  See `tf.dtypes` for a complete list of `DType`'s defined.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unable to convert function return value to a Python type! The signature was\n\t() -> handle"
     ]
    }
   ],
   "source": [
    "# import tensorflow and keras for padding \n",
    "import tensorflow as tf \n",
    "import keras as k\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras_preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>ArticleTitle</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>volume</th>\n",
       "      <th>date</th>\n",
       "      <th>pubmed</th>\n",
       "      <th>doi_pii_str</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B. Jiang, Y. Shi, Y. Peng, Y. Jia, Y. Yan, X. ...</td>\n",
       "      <td>\"Cold-Induced CBF-PIF3 Interaction Enhances Fr...</td>\n",
       "      <td>Molecular plant</td>\n",
       "      <td>13(6)</td>\n",
       "      <td>(Jun. 2020).</td>\n",
       "      <td>PUBMED: 32311530;</td>\n",
       "      <td>DOI 10.1016/j.molp.2020.04.006.</td>\n",
       "      <td>Growth inhibition and cold-acclimation strateg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Z. Mao, S. He, F. Xu, X. Wei, L. Jiang, Y. Liu...</td>\n",
       "      <td>\"Photoexcited CRY1 and phyB interact directly ...</td>\n",
       "      <td>The New phytologist</td>\n",
       "      <td>225(2)</td>\n",
       "      <td>(Jan. 2020).</td>\n",
       "      <td>PUBMED: 31514232;</td>\n",
       "      <td>DOI 10.1111/nph.16194.</td>\n",
       "      <td>Arabidopsis CRY1 and phyB are the primary blue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F. Zheng, Y. Wang, D. Gu and X. Liu,</td>\n",
       "      <td>\"Histone Deacetylase HDA15 Restrains PHYB-Depe...</td>\n",
       "      <td>Cells</td>\n",
       "      <td>11(23)</td>\n",
       "      <td>(Nov. 2022).</td>\n",
       "      <td>PUBMED: 36497048;</td>\n",
       "      <td>DOI 10.3390/cells11233788.</td>\n",
       "      <td>Seed germination is essential for the coloniza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JH. Jung, M. Domijan, C. Klose, S. Biswas, D. ...</td>\n",
       "      <td>\"Phytochromes function as thermosensors in Ara...</td>\n",
       "      <td>Science (New York, N.Y.)</td>\n",
       "      <td>354(6314)</td>\n",
       "      <td>(Nov. 2016).</td>\n",
       "      <td>PUBMED: 27789797;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plants are responsive to temperature, and some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T. Yan, Y. Heng, W. Wang, J. Li and XW. Deng,</td>\n",
       "      <td>\"SWELLMAP 2, a phyB-Interacting Splicing Facto...</td>\n",
       "      <td>Frontiers in plant science</td>\n",
       "      <td>13</td>\n",
       "      <td>(2022).</td>\n",
       "      <td>PUBMED: 35222493;</td>\n",
       "      <td>DOI 10.3389/fpls.2022.836519.</td>\n",
       "      <td>Light-triggered transcriptome reprogramming is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             authors  \\\n",
       "0  B. Jiang, Y. Shi, Y. Peng, Y. Jia, Y. Yan, X. ...   \n",
       "1  Z. Mao, S. He, F. Xu, X. Wei, L. Jiang, Y. Liu...   \n",
       "2              F. Zheng, Y. Wang, D. Gu and X. Liu,    \n",
       "3  JH. Jung, M. Domijan, C. Klose, S. Biswas, D. ...   \n",
       "4     T. Yan, Y. Heng, W. Wang, J. Li and XW. Deng,    \n",
       "\n",
       "                                        ArticleTitle  \\\n",
       "0  \"Cold-Induced CBF-PIF3 Interaction Enhances Fr...   \n",
       "1  \"Photoexcited CRY1 and phyB interact directly ...   \n",
       "2  \"Histone Deacetylase HDA15 Restrains PHYB-Depe...   \n",
       "3  \"Phytochromes function as thermosensors in Ara...   \n",
       "4  \"SWELLMAP 2, a phyB-Interacting Splicing Facto...   \n",
       "\n",
       "                 journal_title      volume           date              pubmed  \\\n",
       "0             Molecular plant       13(6)   (Jun. 2020).   PUBMED: 32311530;    \n",
       "1         The New phytologist      225(2)   (Jan. 2020).   PUBMED: 31514232;    \n",
       "2                       Cells      11(23)   (Nov. 2022).   PUBMED: 36497048;    \n",
       "3    Science (New York, N.Y.)   354(6314)   (Nov. 2016).   PUBMED: 27789797;    \n",
       "4  Frontiers in plant science          13        (2022).   PUBMED: 35222493;    \n",
       "\n",
       "                       doi_pii_str  \\\n",
       "0  DOI 10.1016/j.molp.2020.04.006.   \n",
       "1           DOI 10.1111/nph.16194.   \n",
       "2       DOI 10.3390/cells11233788.   \n",
       "3                              NaN   \n",
       "4    DOI 10.3389/fpls.2022.836519.   \n",
       "\n",
       "                                            abstract  \n",
       "0  Growth inhibition and cold-acclimation strateg...  \n",
       "1  Arabidopsis CRY1 and phyB are the primary blue...  \n",
       "2  Seed germination is essential for the coloniza...  \n",
       "3  Plants are responsive to temperature, and some...  \n",
       "4  Light-triggered transcriptome reprogramming is...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from phyb_400 csv \n",
    "df_original = pd.read_csv(\"/Users/emilyjiang/Desktop/Webscraping/PHYB_400.csv\", index_col=0) \n",
    "\n",
    "# print beginning of data to check \n",
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking shape of dataset \n",
    "df_original.shape\n",
    "# dataset has 8 columns, 400 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "authors            1\n",
       "ArticleTitle       0\n",
       "journal_title      0\n",
       "volume             7\n",
       "date               0\n",
       "pubmed             0\n",
       "doi_pii_str      128\n",
       "abstract           5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if  dataset has any missing values\n",
    "df_original.isnull().sum()\n",
    "\n",
    "# check what to do with missing values? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ciaran's next code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# import dataset\n",
    "df_orig = pd.read_csv(\"PHYB_400.csv\", index_col=0)\n",
    "# download a list of useless stop words\n",
    "df_orig.iloc[:,-1] = df_orig.iloc[:,-1].astype(str)\n",
    "stop_words = set(stopwords.words('english')) \n",
    "# nltk.download('stopwords')\n",
    "# print(nltk.download('stopwords'))\n",
    "# print(filter(lambda w: not w in s, df.iloc[1,-1]))\n",
    "# stop_words\n",
    "df_orig\n",
    "\n",
    "# tokenize the words (make the paragraph into a list of strings for each word)\n",
    "df = df_orig\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    df.iloc[i,-1] = word_tokenize(df.iloc[i,-1])\n",
    "    # df.iloc[i,-1] = \"\".join([word + \" \" for word in text_tokens if not word in stopwords.words()])\n",
    "# convert to the lsit to a string\n",
    "df.iloc[:,-1] = df.iloc[:,-1].astype(str)\n",
    "# strip bad characters\n",
    "bad_letters = ['\\'', '.', ',', '[', ']', '(', ')']\n",
    "for i in bad_letters:\n",
    "    df.iloc[:,-1] = df.iloc[:,-1].map(lambda x: x.replace(i,''))\n",
    "# remove multiple spaces\n",
    "for i in range(5):  \n",
    "    df.iloc[:,-1] = df.iloc[:,-1].map(lambda x: x.replace('  ',' '))\n",
    "print(df.iloc[0,-1])\n",
    "\n",
    "#Tokenize the sentences (but using keras this time)\n",
    "myTokenizer = Tokenizer(num_words=1000)\n",
    "myTokenizer.fit_on_texts(df.iloc[:,-1])\n",
    "sequences = myTokenizer.texts_to_sequences(df.iloc[:,-1])\n",
    "\n",
    "# Padding ( make everything the same dimension by adding zeros to the front)\n",
    "padded = pad_sequences(sequences, maxlen=len(df.iloc[:,-1][3].split(\" \")))\n",
    "\n",
    "df['Count'] = df['abstract'].map(lambda x: x.count(\"phyB\"))\n",
    "# df[df['Count'] > 0]\n",
    "df['Label'] = sum([df['Count'] > 3])'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking line to read abstract of article from Ciaran's model (***CHECK THAT THAT LINE READS ABSTRACT AS STRING***): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Growth inhibition and cold-acclimation strateg...\n",
      "1      Arabidopsis CRY1 and phyB are the primary blue...\n",
      "2      Seed germination is essential for the coloniza...\n",
      "3      Plants are responsive to temperature, and some...\n",
      "4      Light-triggered transcriptome reprogramming is...\n",
      "                             ...                        \n",
      "395    The phytochrome family of red/far-red (R/FR)-r...\n",
      "396    Phytochromes are red (R) and far-red (FR) ligh...\n",
      "397    The phytochrome (phy) family of sensory photor...\n",
      "398    Light is a crucial environmental signal that c...\n",
      "399    Overexpression of phytochrome B (phyB) in Arab...\n",
      "Name: abstract, Length: 400, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_original.iloc[:,-1] = df_original.iloc[:,-1].astype(str)\n",
    "print(df_original.iloc[:,-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using text_cleaning() function to clean dataset of text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words =  stopwords.words('english')\n",
    "def text_cleaning(text, remove_stop_words=True, lemmatize_words=True):\n",
    "    # Clean the text, with the option to remove stop_words and to lemmatize word\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text =  re.sub(r'http\\S+',' link ', text)\n",
    "    text = re.sub(r'\\b\\d+(?:\\.\\d+)?\\s+', '', text) # remove numbers\n",
    "        \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stop_words:\n",
    "        text = text.split()\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if lemmatize_words:\n",
    "        text = text.split()\n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in text]\n",
    "        text = \" \".join(lemmatized_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use text_cleaning() function to clean data, accessing last column of dataframe (which is the abstract)\n",
    "Creates a new column for cleaned data. Using text_cleaning function from above chunk of code, we are cleaning the text in the abstract column and saving this cleaned text in a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>ArticleTitle</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>volume</th>\n",
       "      <th>date</th>\n",
       "      <th>pubmed</th>\n",
       "      <th>doi_pii_str</th>\n",
       "      <th>abstract</th>\n",
       "      <th>cleaned_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B. Jiang, Y. Shi, Y. Peng, Y. Jia, Y. Yan, X. ...</td>\n",
       "      <td>\"Cold-Induced CBF-PIF3 Interaction Enhances Fr...</td>\n",
       "      <td>Molecular plant</td>\n",
       "      <td>13(6)</td>\n",
       "      <td>(Jun. 2020).</td>\n",
       "      <td>PUBMED: 32311530;</td>\n",
       "      <td>DOI 10.1016/j.molp.2020.04.006.</td>\n",
       "      <td>Growth inhibition and cold-acclimation strateg...</td>\n",
       "      <td>Growth inhibition and cold-acclimation strateg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Z. Mao, S. He, F. Xu, X. Wei, L. Jiang, Y. Liu...</td>\n",
       "      <td>\"Photoexcited CRY1 and phyB interact directly ...</td>\n",
       "      <td>The New phytologist</td>\n",
       "      <td>225(2)</td>\n",
       "      <td>(Jan. 2020).</td>\n",
       "      <td>PUBMED: 31514232;</td>\n",
       "      <td>DOI 10.1111/nph.16194.</td>\n",
       "      <td>Arabidopsis CRY1 and phyB are the primary blue...</td>\n",
       "      <td>Arabidopsis CRY1 and phyB are the primary blue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F. Zheng, Y. Wang, D. Gu and X. Liu,</td>\n",
       "      <td>\"Histone Deacetylase HDA15 Restrains PHYB-Depe...</td>\n",
       "      <td>Cells</td>\n",
       "      <td>11(23)</td>\n",
       "      <td>(Nov. 2022).</td>\n",
       "      <td>PUBMED: 36497048;</td>\n",
       "      <td>DOI 10.3390/cells11233788.</td>\n",
       "      <td>Seed germination is essential for the coloniza...</td>\n",
       "      <td>Seed germination is essential for the coloniza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JH. Jung, M. Domijan, C. Klose, S. Biswas, D. ...</td>\n",
       "      <td>\"Phytochromes function as thermosensors in Ara...</td>\n",
       "      <td>Science (New York, N.Y.)</td>\n",
       "      <td>354(6314)</td>\n",
       "      <td>(Nov. 2016).</td>\n",
       "      <td>PUBMED: 27789797;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plants are responsive to temperature, and some...</td>\n",
       "      <td>Plants are responsive to temperature, and some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T. Yan, Y. Heng, W. Wang, J. Li and XW. Deng,</td>\n",
       "      <td>\"SWELLMAP 2, a phyB-Interacting Splicing Facto...</td>\n",
       "      <td>Frontiers in plant science</td>\n",
       "      <td>13</td>\n",
       "      <td>(2022).</td>\n",
       "      <td>PUBMED: 35222493;</td>\n",
       "      <td>DOI 10.3389/fpls.2022.836519.</td>\n",
       "      <td>Light-triggered transcriptome reprogramming is...</td>\n",
       "      <td>Light-triggered transcriptome reprogramming is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             authors  \\\n",
       "0  B. Jiang, Y. Shi, Y. Peng, Y. Jia, Y. Yan, X. ...   \n",
       "1  Z. Mao, S. He, F. Xu, X. Wei, L. Jiang, Y. Liu...   \n",
       "2              F. Zheng, Y. Wang, D. Gu and X. Liu,    \n",
       "3  JH. Jung, M. Domijan, C. Klose, S. Biswas, D. ...   \n",
       "4     T. Yan, Y. Heng, W. Wang, J. Li and XW. Deng,    \n",
       "\n",
       "                                        ArticleTitle  \\\n",
       "0  \"Cold-Induced CBF-PIF3 Interaction Enhances Fr...   \n",
       "1  \"Photoexcited CRY1 and phyB interact directly ...   \n",
       "2  \"Histone Deacetylase HDA15 Restrains PHYB-Depe...   \n",
       "3  \"Phytochromes function as thermosensors in Ara...   \n",
       "4  \"SWELLMAP 2, a phyB-Interacting Splicing Facto...   \n",
       "\n",
       "                 journal_title      volume           date              pubmed  \\\n",
       "0             Molecular plant       13(6)   (Jun. 2020).   PUBMED: 32311530;    \n",
       "1         The New phytologist      225(2)   (Jan. 2020).   PUBMED: 31514232;    \n",
       "2                       Cells      11(23)   (Nov. 2022).   PUBMED: 36497048;    \n",
       "3    Science (New York, N.Y.)   354(6314)   (Nov. 2016).   PUBMED: 27789797;    \n",
       "4  Frontiers in plant science          13        (2022).   PUBMED: 35222493;    \n",
       "\n",
       "                       doi_pii_str  \\\n",
       "0  DOI 10.1016/j.molp.2020.04.006.   \n",
       "1           DOI 10.1111/nph.16194.   \n",
       "2       DOI 10.3390/cells11233788.   \n",
       "3                              NaN   \n",
       "4    DOI 10.3389/fpls.2022.836519.   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Growth inhibition and cold-acclimation strateg...   \n",
       "1  Arabidopsis CRY1 and phyB are the primary blue...   \n",
       "2  Seed germination is essential for the coloniza...   \n",
       "3  Plants are responsive to temperature, and some...   \n",
       "4  Light-triggered transcriptome reprogramming is...   \n",
       "\n",
       "                                        cleaned_data  \n",
       "0  Growth inhibition and cold-acclimation strateg...  \n",
       "1  Arabidopsis CRY1 and phyB are the primary blue...  \n",
       "2  Seed germination is essential for the coloniza...  \n",
       "3  Plants are responsive to temperature, and some...  \n",
       "4  Light-triggered transcriptome reprogramming is...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_original[\"cleaned_data\"] = df_original[df_original.columns[len(df_original.columns)-1]]\n",
    "df_original.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing data, padding data, counting occurences of \"phyB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/emilyjiang/Desktop/Git/Phoenix Bioinformatics/emily_MNB_NLP.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emilyjiang/Desktop/Git/Phoenix%20Bioinformatics/emily_MNB_NLP.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# tokenize the sentences (but using keras this time)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/emilyjiang/Desktop/Git/Phoenix%20Bioinformatics/emily_MNB_NLP.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m myTokenizer \u001b[39m=\u001b[39m Tokenizer(num_words\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emilyjiang/Desktop/Git/Phoenix%20Bioinformatics/emily_MNB_NLP.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m myTokenizer\u001b[39m.\u001b[39mfit_on_texts(df_original\u001b[39m.\u001b[39miloc[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emilyjiang/Desktop/Git/Phoenix%20Bioinformatics/emily_MNB_NLP.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m sequences \u001b[39m=\u001b[39m myTokenizer\u001b[39m.\u001b[39mtexts_to_sequences(df_original\u001b[39m.\u001b[39miloc[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# tokenize the sentences (but using keras this time)\n",
    "myTokenizer = Tokenizer(num_words=1000)\n",
    "myTokenizer.fit_on_texts(df_original.iloc[:,-1])\n",
    "sequences = myTokenizer.texts_to_sequences(df_original.iloc[:,-1])\n",
    "\n",
    "# padding (make everything the same dimension by adding zeros to the front)\n",
    "padded = pad_sequences(sequences, maxlen=len(df_original.iloc[:,-1][3].split(\" \")))\n",
    "\n",
    "# count number of \"phyB\" occurrences in cleaned text column \n",
    "df_original['count'] = df_original[df_original.columns[len(df_original.columns)-1]].map(lambda x: x.count(\"phyB\"))\n",
    "# df_original[df_original['count'] > 0]\n",
    "df_original['label'] = sum([df_original['count'] > 3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into feature and target variables: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
      "\u001b[1;32m/Users/emilyjiang/Untitled-1.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emilyjiang/Untitled-1.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m x \u001b[39m=\u001b[39m df_original[\u001b[39m\"\u001b[39m\u001b[39mcleaned_data\u001b[39m\u001b[39m\"\u001b[39m] \n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/emilyjiang/Untitled-1.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m y \u001b[39m=\u001b[39m df_original\u001b[39m.\u001b[39;49msentiment\u001b[39m.\u001b[39mvalues\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n",
      "\u001b[1;32m   5568\u001b[0m \u001b[39mif\u001b[39;00m (\n",
      "\u001b[1;32m   5569\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n",
      "\u001b[1;32m   5570\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n",
      "\u001b[1;32m   5571\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n",
      "\u001b[1;32m   5572\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n",
      "\u001b[1;32m   5573\u001b[0m ):\n",
      "\u001b[1;32m   5574\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n",
      "\u001b[0;32m-> 5575\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'sentiment'"
     ]
    }
   ],
   "source": [
    "x = df_original[\"cleaned_data\"] \n",
    "y = df_original.sentiment.values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into train and test data, test = 15% of dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[1;32m/Users/emilyjiang/Untitled-1.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/emilyjiang/Untitled-1.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m x_train, x_valid, y_train, y_valid \u001b[39m=\u001b[39m train_test_split(x, y, \n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emilyjiang/Untitled-1.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_size\u001b[39m=\u001b[39m\u001b[39m0.15\u001b[39m,\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emilyjiang/Untitled-1.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, \n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emilyjiang/Untitled-1.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emilyjiang/Untitled-1.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m stratify\u001b[39m=\u001b[39my,\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emilyjiang/Untitled-1.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, \n",
    "test_size=0.15,\n",
    "random_state=42, \n",
    "shuffle=True,\n",
    "stratify=y,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating NLP Model: Now training Multinomial Naive Bayes algorithm (common algorithm for text classification) to classify if article is about phyB or not"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform cleaned data into numerical values so model can understand it. Using TfidfVectorizer method from scikit-learn (converts text documents to matrix of TF-IDF features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Pipeline class from scikit-learn to apply list of transforms and final estimator for pre-processing and training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phyB_classifier = Pipeline(steps=[\n",
    "    ('pre_processing',TfidfVectorizer(lowercase=False)),\n",
    "    ('naive_bayes',MultinomialNB())\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phyB_classifier.fit(x_train, y_train) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictiom from validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = phyB_classifier.predict(x_valid) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_valid, y_predict) \n",
    "print(accuracy_score) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# import it to the webscraping folder\n",
    "joblib.dump(phyB_classifier, '/Users/emilyjiang/Desktop/Webscraping')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdec36b3ce009a3c34851a12eb9b6c73eae6306fd6680792593ee6082769bc5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
